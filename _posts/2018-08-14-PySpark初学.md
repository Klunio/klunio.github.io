---
layout: single
author_profile: true
title:  "Spark Note"
date:   2019-08-14 14:00:40 +0800
categories: note
classes: wide
toc: true
toc_label: "文章结构"
toc_icon: "align-left"
---



Apache Spark 是目前处理和使用大数据的最广泛使用的框架之一，Python是数据分析、机器学习等最广泛使用的编程语言之一。为了用Spark支持Python，Apache Spark社区发布了一个工具PySpark。使用PySpark，您也可以使用Python编程语言处理RDD。正是由于一个名为Py4j的库，他们才能实现这一目标。

Apache Spark是Apache Software Foundation开发的用于实时处理的开源集群计算框架。 Spark提供了一个接口，用于编程具有隐式数据并行和容错功能的集群。![1530345646429785](https://www.itcodemonkey.com/data/upload/portal/20180630/1530345646429785.png)

### SparkContent

SparkContext是任何spark功能的入口点。当我们运行任何Spark应用程序时，会启动一个驱动程序，它具有main函数，并且此处启动了SparkContext。然后，驱动程序在工作节点上的执行程序内运行操作。

SparkContext使用Py4J启动JVM并创建JavaSparkContext。默认情况下，PySpark将SparkContext作为'sc'提供，因此创建新的SparkContext将不起作用。

![img](https://upload-images.jianshu.io/upload_images/1531909-017acff244e3caf2.png?imageMogr2/auto-orient/)

以下代码块包含PySpark类的详细信息以及SparkContext可以采用的参数。

```python
class pyspark.SparkContext (
   master = None,
   appName = None, 
   sparkHome = None, 
   pyFiles = None, 
   environment = None, 
   batchSize = 0, 
   serializer = PickleSerializer(), 
   conf = None, 
   gateway = None, 
   jsc = None, 
   profiler_cls = <class 'pyspark.profiler.BasicProfiler'>
)
```

以下是SparkContext的参数具体含义：

-  `Master`- 它是连接到的集群的URL。
-  `appName`- 您的工作名称。
-  `sparkHome` - Spark安装目录。
-  `pyFiles` - 要发送到集群并添加到PYTHONPATH的.zip或.py文件。
-  `environment` - 工作节点环境变量。
-  `batchSize` - 表示为单个Java对象的Python对象的数量。设置1以禁用批处理，设置0以根据对象大小自动选择批处理大小，或设置为-1以使用无限批处理大小。
-  `serializer`- RDD序列化器。
-  `Conf` - L {SparkConf}的一个对象，用于设置所有Spark属性。
-  `gateway`  - 使用现有网关和JVM，否则初始化新JVM。
-  `JSC` - JavaSparkContext实例。
-  `profiler_cls` - 用于进行性能分析的一类自定义Profiler（默认为pyspark.profiler.BasicProfiler）。
   在上述参数中，主要使用master和appname。任何PySpark程序的会使用以下两行：

```
from pyspark import SparkContext
sc = SparkContext("local", "First App")
```



### RDD

在介绍PySpark处理RDD操作之前，我们先了解下RDD的基本概念：

> RDD代表Resilient Distributed Dataset，它们是在多个节点上运行和操作以在集群上进行并行处理的元素。RDD是不可变元素，这意味着一旦创建了RDD，就无法对其进行更改。RDD也具有容错能力，因此在发生任何故障时，它们会自动恢复。您可以对这些RDD应用多个操作来完成某项任务。

要对这些RDD进行操作，有两种方法 :

- Transformation
- Action

转换 - 这些操作应用于RDD以创建新的RDD。Filter，groupBy和map是转换的示例。

操作 - 这些是应用于RDD的操作，它指示Spark执行计算并将结果发送回驱动程序。

要在PySpark中应用任何操作，我们首先需要创建一个PySpark RDD。以下代码块具有PySpark RDD类的详细信息 :

```python
class pyspark.RDD (
   jrdd, 
   ctx, 
   jrdd_deserializer = AutoBatchedSerializer(PickleSerializer())
)
```

接下来让我们看看如何使用PySpark运行一些基本操作,用以下代码创建存储一组单词的RDD（`spark使用parallelize方法创建RDD`），我们现在将对单词进行一些操作。



### PySpark SQL

PySpark的语法是从左到右串行的，便于阅读、理解和修正；SQL的语法是从内到外嵌套的，不方便维护；

PySpark继承Python优美、简洁的语法，同样的效果，代码行数可能只有SQL的十分之一；

Spark分转化操作和行动操作，只在行动操作时才真正计算，所以可以减少不必要的计算时间；

相对于SQL层层嵌套的一个整体，PySpark可以拆分成多步，并可以十分方便地把中间结果保存为变量，更有利于调试和修改；

PySpark可以与Python中的其他模块结合使用，可以将多种功能有机结合成一个系统

PySpark SQL模块许多函数、方法与SQL中关键字一样，可以以比较低的学习成本切换

最重要的，Spark是基于内存计算的，计算速度本身比Hive快很多倍



refer：https://www.jianshu.com/p/177cbcb1cb6f

